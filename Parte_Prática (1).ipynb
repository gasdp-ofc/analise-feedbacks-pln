{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOPeUg5BfvI4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Instalação das bibliotecas necessárias\n",
        "!pip install -q nltk spacy gensim scikit-learn matplotlib\n",
        "!python -m spacy download pt_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QR3cgq3qiMhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import spacy\n",
        "\n",
        "# Downloads necessários do NLTK\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"rslp\")\n",
        "\n",
        "# Carregamento de recursos\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "stopwords_pt = set(stopwords.words(\"portuguese\"))\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "# -----------------------------\n",
        "# Base de dados simulada\n",
        "# -----------------------------\n",
        "dados = [\n",
        "    (\"Entrega muito rápida, adorei o produto!\", \"positivo\"),\n",
        "    (\"Produto chegou quebrado e com defeito.\", \"negativo\"),\n",
        "    (\"Demorou demais para chegar.\", \"negativo\"),\n",
        "    (\"Atendimento excelente e muito educado.\", \"positivo\"),\n",
        "    (\"Embalagem rasgada, mas produto ok.\", \"neutro\"),\n",
        "    (\"Preço justo e entrega no prazo.\", \"positivo\"),\n",
        "    (\"Suporte demorou para responder.\", \"negativo\"),\n",
        "    (\"Qualidade ótima, recomendo.\", \"positivo\"),\n",
        "    (\"Não funcionou corretamente.\", \"negativo\"),\n",
        "    (\"Experiência normal, nada demais.\", \"neutro\"),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(dados, columns=[\"texto\", \"sentimento\"])\n",
        "\n",
        "# -----------------------------\n",
        "# Funções de limpeza\n",
        "# -----------------------------\n",
        "def limpar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r\"http\\S+\", \"\", texto)\n",
        "    texto = unicodedata.normalize(\"NFKD\", texto)\n",
        "    texto = \"\".join(c for c in texto if not unicodedata.combining(c))\n",
        "    texto = texto.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return texto\n",
        "\n",
        "def preprocessar(texto):\n",
        "    texto_limpo = limpar_texto(texto)\n",
        "    tokens = texto_limpo.split()\n",
        "    tokens = [t for t in tokens if t not in stopwords_pt and len(t) > 2]\n",
        "    stems = [stemmer.stem(t) for t in tokens]\n",
        "    lemas = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
        "    return texto_limpo, tokens, stems, lemas\n",
        "\n",
        "# Aplicação do pré-processamento\n",
        "df[\"texto_limpo\"], df[\"tokens\"], df[\"stems\"], df[\"lemas\"] = zip(\n",
        "    *df[\"texto\"].apply(preprocessar)\n",
        ")\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "JXDEPRILgASW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(df[\"texto_limpo\"])\n",
        "\n",
        "# Redução de dimensionalidade\n",
        "svd = TruncatedSVD(n_components=2, random_state=42)\n",
        "X_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "# Word2Vec\n",
        "w2v = Word2Vec(\n",
        "    sentences=df[\"tokens\"],\n",
        "    vector_size=50,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "w2v.wv.most_similar(\"entrega\")\n"
      ],
      "metadata": {
        "id": "A79zVgSVgL-f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Codificação dos rótulos\n",
        "mapa = {\"negativo\": 0, \"neutro\": 1, \"positivo\": 2}\n",
        "df[\"y\"] = df[\"sentimento\"].map(mapa)\n",
        "\n",
        "# Divisão treino/teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, df[\"y\"], test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Modelo\n",
        "modelo = LogisticRegression(max_iter=1000)\n",
        "modelo.fit(X_train, y_train)\n",
        "\n",
        "# Avaliação\n",
        "y_pred = modelo.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "gN5-dRWXgTbZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# -----------------------------\n",
        "# LDA – Tópicos\n",
        "# -----------------------------\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform(df[\"texto_limpo\"])\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
        "lda.fit(X_count)\n",
        "\n",
        "print(\"Tópicos identificados:\")\n",
        "for i, topic in enumerate(lda.components_):\n",
        "    palavras = [count_vec.get_feature_names_out()[j] for j in topic.argsort()[-6:]]\n",
        "    print(f\"Tópico {i}: {', '.join(palavras)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Clustering\n",
        "# -----------------------------\n",
        "kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
        "df[\"cluster\"] = kmeans.fit_predict(X_svd)\n",
        "\n",
        "# -----------------------------\n",
        "# Gráficos\n",
        "# -----------------------------\n",
        "plt.figure()\n",
        "df[\"sentimento\"].value_counts().plot(kind=\"bar\", title=\"Distribuição de Sentimentos\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "for c in df[\"cluster\"].unique():\n",
        "    idx = df[\"cluster\"] == c\n",
        "    plt.scatter(X_svd[idx, 0], X_svd[idx, 1], label=f\"Cluster {c}\")\n",
        "plt.legend()\n",
        "plt.title(\"Clusters de Feedbacks\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rd3-LDV7gYzZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}